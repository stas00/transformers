#!/usr/bin/env python
# coding: utf-8

import os, sys
sys.path.insert(0, f"{os.getcwd()}/src")


import torch
from pprint import pprint
import fairseq


def dump_state_keys(state_dict): print("\n".join(state_dict.keys()))


# # Baseline

from fairseq import hub_utils
#checkpoint_file = 'model1.pt:model2.pt:model3.pt:model4.pt'
checkpoint_file = 'model1.pt'
model_name_or_path = 'transformer.wmt19.ru-en'
data_name_or_path = '.'
cls = fairseq.model_parallel.models.transformer.ModelParallelTransformerModel
models = cls.hub_models()
kwargs = {'bpe': 'fastbpe', 'tokenizer': 'moses'}

ru2en = hub_utils.from_pretrained(
            model_name_or_path,
            checkpoint_file,
            data_name_or_path,
            archive_map=models,
            **kwargs
        )


# In[7]:


#checkpoint_file='model1.pt:model2.pt:model3.pt:model4.pt'
# path = "/code/huggingface/transformers-fair-wmt"
# mname = 'transformer.wmt19.ru-en'
# chkpt1_path = path + "/data/wmt19.ru-en.ensemble/model4.pt"
# checkpoint_file='model1.pt'

# chkpt = torch.load(chkpt1_path, map_location="cpu")


# validate that the model keys are correct:
#hub_interface = torch.hub.load("pytorch/fairseq", mname, checkpoint_file).eval()
#hub_interface.models[0].load_state_dict(chkpt["model"])
#chkpt = hub_interface

# this works just as well, but the above validates that the local copy loads correctly
# chkpt = torch.load(chkpt1_path, map_location="cpu")


# In[8]:


#chkpt.models[0].upgrade_state_dict(chkpt.models[0].state_dict())


# In[9]:


model = ru2en["models"][0]
model


# In[10]:


args = dict(vars(ru2en["args"]))


# In[11]:


args["source_lang"]
args["encoder_embed_dim"]
args["decoder_embed_dim"]


# In[12]:


pprint(args)


# In[13]:


pprint(args.keys())


# In[14]:


model_state_dict = model.state_dict()
#model_state_dict


# In[15]:


#model = dict(vars(model))
dump_state_keys(model_state_dict)


# In[16]:


#model.items()
model_state_dict["decoder.layers.5.fc2.bias"].shape.numel()
model_state_dict["decoder.layers.5.fc2.bias"].shape[0]


# In[17]:


#list(model.parameters())


# In[18]:


# dump the state_dict attrs and their shape
pprint([f"{' '.join(map(str, v.shape)):>12} {k}"for k,v in model_state_dict.items()])


# In[ ]:





# In[19]:


# renames/removal
from collections import OrderedDict

rename_keys = [
#    ("model.encoder.embed_positions._float_tensor", "model.encoder.embed_positions.weight"),
#    ("model.decoder.embed_positions._float_tensor", "model.decoder.embed_positions.weight"),
#    ("", ""),
#    ("", ""),
#    ("", ""),
#    ("", ""),    
]

def remove_ignore_keys_(model_state_dict):
    ignore_keys = [
        "model.model",
        "model.encoder.version",
        "model.decoder.version",
#        "model.encoder.embed_positions._float_tensor", # not storing model.encoder.embed_positions.weight
#        "model.decoder.embed_positions._float_tensor", # not storing model.decoder.embed_positions.weight
    ]
    for k in ignore_keys:
        model_state_dict.pop(k, None)

def rename_key(dct, old, new):
    val = dct.pop(old)
    dct[new] = val

#model_state_dict = chkpt["model"].copy()

# rename keys to start with model.
model_state_dict_new = OrderedDict(("model."+k, v) for k, v in model_state_dict.items())
# check:
#model_state_dict["model.encoder.layers.0.fc1.bias"]
#chkpt["model"]["encoder.layers.0.fc1.bias"]
    
remove_ignore_keys_(model_state_dict_new)
for src, dest in rename_keys:
    rename_key(model_state_dict_new, src, dest)
    


# In[20]:


model_state_dict_new["model.decoder.embed_tokens.weight"].shape


# In[21]:


# emulate non-existing layer - perhaps it'll be removed instead in the model - for now just a bias of 0's
model_state_dict_new["final_logits_bias"] = torch.zeros((1, model_state_dict_new["model.decoder.embed_tokens.weight"].shape[0])) 
                                                        
model_state_dict_new["final_logits_bias"].shape


# In[22]:


#pprint(state_dict.keys())
#state_dict["model.encoder.layers.0.fc1.bias"]
#state_dict["model.encoder.embed_positions.weight"]
#torch.FloatTensor(1)


# # This is it

# In[42]:


from transformers.modeling_fsmt import FSMTForConditionalGeneration
from transformers.configuration_fsmt import FSMTConfig


# In[43]:


hf_checkpoint_name = "/code/huggingface/transformers-fair-wmt/data/wmt19-ru-en/config.json"
config = FSMTConfig.from_pretrained(hf_checkpoint_name)
model_new = FSMTForConditionalGeneration(config).eval()
#state_dict = chkpt["model"]


# In[44]:


#dump_state_keys(model_state_dict_new)
model_state_dict_new["model.decoder.embed_tokens.weight"].shape



# let's add dummy things so that load_state_dict doesn't complain
# (embed_positions): SinusoidalPositionalEmbedding(1024, 1024)
# XXX: these seem to be autogenerated on the fly, no need to store
#model_state_dict_new["model.encoder.embed_positions.weight"] = model_state_dict_new["model.decoder.embed_positions.weight"] = torch.zeros((args["decoder_input_dim"], args["decoder_input_dim"]))

#model_state_dict_new["model.encoder.embed_positions.weight"].shape
#model_state_dict_new["model.decoder.embed_positions.weight"].shape

# these too get autogenerated:
# "model.encoder_embed_tokens.weight",
# "model.decoder_embed_tokens.weight",

# encoder_emd_tok_dim
args["encoder_emd_tok_dim"] = 31232
args["decoder_emd_tok_dim"] = 31640

model_state_dict_new["model.encoder_embed_tokens.weight"] = torch.zeros((args["encoder_emd_tok_dim"], args["encoder_embed_dim"]))

model_state_dict_new["model.decoder_embed_tokens.weight"] = torch.zeros((args["decoder_emd_tok_dim"], args["decoder_embed_dim"]))

model_state_dict_new["model.encoder_embed_tokens.weight"].shape
model_state_dict_new["model.decoder_embed_tokens.weight"].shape


# In[46]:


# show missing or extraneous/mismatching keys (need to remap/change model to match)
model_new.load_state_dict(model_state_dict_new)


model_new


from transformers.tokenization_fsmt import FSMTTokenizer
tokenizer = FSMTTokenizer.from_pretrained('fsmt-wmt19-ru-en')

model_new.eval()

sentence = "Машинное обучение - это здорово! Ты молодец."

input_ids = tokenizer.encode(sentence, return_tensors='pt')
print(input_ids)
outputs = model_new.generate(input_ids)
print(outputs)
decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(decoded)

