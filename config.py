#!/usr/bin/env python
# coding: utf-8

import os, sys
sys.path.insert(0, f"{os.getcwd()}/src")


import torch
from pprint import pprint
import fairseq


def dump_state_keys(state_dict): print("\n".join(state_dict.keys()))


# # Baseline

#checkpoint_file='model1.pt:model2.pt:model3.pt:model4.pt'
checkpoint_file='model1.pt'
ru2en = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.ru-en', checkpoint_file=checkpoint_file, tokenizer='moses', bpe='fastbpe')


# from fairseq import hub_utils
# #checkpoint_file = 'model1.pt:model2.pt:model3.pt:model4.pt'
# checkpoint_file = 'model1.pt'
# model_name_or_path = 'transformer.wmt19.ru-en'
# data_name_or_path = '.'
# cls = fairseq.model_parallel.models.transformer.ModelParallelTransformerModel
# models = cls.hub_models()
# kwargs = {'bpe': 'fastbpe', 'tokenizer': 'moses'}

# ru2en = hub_utils.from_pretrained(
#             model_name_or_path,
#             checkpoint_file,
#             data_name_or_path,
#             archive_map=models,
#             **kwargs
#         )



model = ru2en.models[0]
model



args = dict(vars(ru2en.args))



args["source_lang"]
args["encoder_embed_dim"]
args["decoder_embed_dim"]

pprint(args)



pprint(args.keys())



model_state_dict = model.state_dict()
#model_state_dict



#model = dict(vars(model))
#dump_state_keys(model_state_dict)



#model.items()
model_state_dict["decoder.layers.5.fc2.bias"].shape.numel()
model_state_dict["decoder.layers.5.fc2.bias"].shape[0]



# dump the state_dict attrs and their shape
#pprint([f"{' '.join(map(str, v.shape)):>12} {k}"for k,v in model_state_dict.items()])


# renames/removal
from collections import OrderedDict

rename_keys = [
#    ("model.encoder.embed_positions._float_tensor", "model.encoder.embed_positions.weight"),
#    ("model.decoder.embed_positions._float_tensor", "model.decoder.embed_positions.weight"),
#    ("", ""),
#    ("", ""),
#    ("", ""),
#    ("", ""),
]

def remove_ignore_keys_(model_state_dict):
    ignore_keys = [
        "model.model",
        "model.encoder.version",
        "model.decoder.version",
        "model.encoder_embed_tokens.weight",
        "model.decoder_embed_tokens.weight",
#        "model.encoder.embed_positions._float_tensor", # not storing model.encoder.embed_positions.weight
#        "model.decoder.embed_positions._float_tensor", # not storing model.decoder.embed_positions.weight
    ]
    for k in ignore_keys:
        model_state_dict.pop(k, None)

def rename_key(dct, old, new):
    val = dct.pop(old)
    dct[new] = val

#model_state_dict = chkpt["model"].copy()

# rename keys to start with model.
model_state_dict_new = OrderedDict(("model."+k, v) for k, v in model_state_dict.items())
# check:
#model_state_dict["model.encoder.layers.0.fc1.bias"]
#chkpt["model"]["encoder.layers.0.fc1.bias"]

remove_ignore_keys_(model_state_dict_new)
for src, dest in rename_keys:
    rename_key(model_state_dict_new, src, dest)

model_state_dict_new["model.decoder.embed_tokens.weight"].shape

# XXX: emulate non-existing layer - perhaps it'll be removed instead in the model - for now just a bias of 0's
model_state_dict_new["final_logits_bias"] = torch.zeros((1, model_state_dict_new["model.decoder.embed_tokens.weight"].shape[0]))

model_state_dict_new["final_logits_bias"].shape


from transformers.modeling_fsmt import FSMTForConditionalGeneration
from transformers.configuration_fsmt import FSMTConfig

#dump_state_keys(model_state_dict_new)
#model_state_dict_new["model.decoder.embed_tokens.weight"].shape



# let's add dummy things so that load_state_dict doesn't complain
# (embed_positions): SinusoidalPositionalEmbedding(1024, 1024)
# XXX: these seem to be autogenerated on the fly, no need to store
#model_state_dict_new["model.encoder.embed_positions.weight"] = model_state_dict_new["model.decoder.embed_positions.weight"] = torch.zeros((args["decoder_input_dim"], args["decoder_input_dim"]))

#model_state_dict_new["model.encoder.embed_positions.weight"].shape
#model_state_dict_new["model.decoder.embed_positions.weight"].shape

# these too get autogenerated:
# "model.encoder_embed_tokens.weight",
# "model.decoder_embed_tokens.weight",

# # encoder_emd_tok_dim
# args["src_vocab_size"] = 31232
# args["tgt_vocab_size"] = 31640
#
# model_state_dict_new["model.encoder_embed_tokens.weight"] = torch.zeros((args["src_vocab_size"], args["encoder_embed_dim"]))
#
# model_state_dict_new["model.decoder_embed_tokens.weight"] = torch.zeros((args["tgt_vocab_size"], args["decoder_embed_dim"]))
#
# model_state_dict_new["model.encoder_embed_tokens.weight"].shape
# model_state_dict_new["model.decoder_embed_tokens.weight"].shape


hf_checkpoint_name = "/code/huggingface/transformers-fair-wmt/data/fsmt-wmt19-ru-en/config.json"
config = FSMTConfig.from_pretrained(hf_checkpoint_name)
model_new = FSMTForConditionalGeneration(config).eval()
#state_dict = chkpt["model"]

import torch
def compare_state_dicts(d1, d2, cmp_func=torch.equal):
    ok = 1
    for k in sorted( set(d1.keys()) | set(d2.keys()) ):
        if k in d1 and k in d2:
            if cmp_func(d1[k], d2[k]):
                pass
            else:
                ok = 0
                print(f"Key {k}: values mismatch: \n{d1[k]}\n{d2[k]}\n")
        else:
            ok = 0
            which = "1st" if k in d2 else "2nd"
            print(f"{which} dict doesn't have key {k}\n")
    if ok:
        print('Models match')
#compare_state_dicts(model_new.state_dict(), model_state_dict_new)

torch.save(model_state_dict_new, "/tmp/new.pt")

# show missing or extraneous/mismatching keys (need to remap/change model to match)
# XXX: somehow this is the key for making the model work
# if I remove this - it stops working
# FSMTForConditionalGeneration probably loads some garbage
model_new.load_state_dict(model_state_dict_new)

model_new


from transformers.tokenization_fsmt import FSMTTokenizer
tokenizer = FSMTTokenizer.from_pretrained('fsmt-wmt19-ru-en')

model_new.eval()

sentence = "Машинное обучение - это здорово! Ты молодец."

input_ids = tokenizer.encode(sentence, return_tensors='pt')
print(input_ids)
outputs = model_new.generate(input_ids)#, num_beams=5)
print("Outputs")
print(outputs)
for output in outputs:
    decoded = tokenizer.decode(output, skip_special_tokens=True)
    print(decoded)
