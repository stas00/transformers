{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(0, f\"{os.getcwd()}/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.18</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from pprint import pprint\n",
    "import pixiedust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint_file='model1.pt:model2.pt:model3.pt:model4.pt'\n",
    "path = \"/code/huggingface/transformers-fair-wmt\"\n",
    "mname = 'transformer.wmt19.ru-en'\n",
    "chkpt1_path = path + \"/data/wmt19.ru-en.ensemble/model4.pt\"\n",
    "checkpoint_file='model1.pt'\n",
    "\n",
    "chkpt = torch.load(chkpt1_path, map_location=\"cpu\")\n",
    "\n",
    "# validate that the model keys are correct:\n",
    "#hub_interface = torch.hub.load(\"pytorch/fairseq\", mname, checkpoint_file).eval()\n",
    "#hub_interface.models[0].load_state_dict(chkpt[\"model\"])\n",
    "#chkpt = hub_interface\n",
    "\n",
    "# this works just as well, but the above validates that the local copy loads correctly\n",
    "# chkpt = torch.load(chkpt1_path, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chkpt.models[0].upgrade_state_dict(chkpt.models[0].state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'clear', 'copy', 'fromkeys', 'get', 'items', 'keys', 'pop', 'popitem', 'setdefault', 'update', 'values']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(chkpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ru'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_orig = dict(vars(chkpt[\"args\"]))\n",
    "#conf_orig\n",
    "conf_orig[\"source_lang\"]\n",
    "conf_orig[\"encoder_embed_dim\"]\n",
    "conf_orig[\"decoder_embed_dim\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adam_betas': '(0.9, 0.98)',\n",
      " 'adam_eps': 1e-08,\n",
      " 'adaptive_input': False,\n",
      " 'adaptive_softmax_cutoff': None,\n",
      " 'adaptive_softmax_dropout': 0,\n",
      " 'arch': 'transformer_wmt_en_de_big',\n",
      " 'attention_dropout': 0.1,\n",
      " 'bucket_cap_mb': 25,\n",
      " 'clip_norm': 0.0,\n",
      " 'cpu': False,\n",
      " 'criterion': 'label_smoothed_cross_entropy',\n",
      " 'data': ['/private/home/edunov/wmt19/data/old/enru',\n",
      "          '/private/home/edunov/wmt19/data/old/enru',\n",
      "          '/private/home/edunov/wmt19/data/finetune/ncru/',\n",
      "          '/private/home/edunov/wmt19/data/temp/enru'],\n",
      " 'ddp_backend': 'c10d',\n",
      " 'decoder_attention_heads': 16,\n",
      " 'decoder_embed_dim': 1024,\n",
      " 'decoder_embed_path': None,\n",
      " 'decoder_ffn_embed_dim': 4096,\n",
      " 'decoder_input_dim': 1024,\n",
      " 'decoder_layers': 6,\n",
      " 'decoder_learned_pos': False,\n",
      " 'decoder_normalize_before': False,\n",
      " 'decoder_output_dim': 1024,\n",
      " 'device_id': 0,\n",
      " 'distributed_backend': 'nccl',\n",
      " 'distributed_init_method': 'tcp://localhost:17185',\n",
      " 'distributed_port': -1,\n",
      " 'distributed_rank': 0,\n",
      " 'distributed_world_size': 2,\n",
      " 'dropout': 0.2,\n",
      " 'encoder_attention_heads': 16,\n",
      " 'encoder_embed_dim': 1024,\n",
      " 'encoder_embed_path': None,\n",
      " 'encoder_ffn_embed_dim': 8192,\n",
      " 'encoder_layers': 6,\n",
      " 'encoder_learned_pos': False,\n",
      " 'encoder_normalize_before': False,\n",
      " 'fix_batches_to_gpus': False,\n",
      " 'fp16': True,\n",
      " 'fp16_init_scale': 128,\n",
      " 'fp16_scale_tolerance': 0.0,\n",
      " 'fp16_scale_window': None,\n",
      " 'keep_interval_updates': -1,\n",
      " 'keep_last_epochs': -1,\n",
      " 'label_smoothing': 0.1,\n",
      " 'lazy_load': False,\n",
      " 'left_pad_source': True,\n",
      " 'left_pad_target': False,\n",
      " 'log_format': 'simple',\n",
      " 'log_interval': 100,\n",
      " 'lr': [0.0007],\n",
      " 'lr_scheduler': 'inverse_sqrt',\n",
      " 'lr_shrink': 0.1,\n",
      " 'max_epoch': 0,\n",
      " 'max_sentences': None,\n",
      " 'max_sentences_valid': None,\n",
      " 'max_source_positions': 1024,\n",
      " 'max_target_positions': 1024,\n",
      " 'max_tokens': 3584,\n",
      " 'max_update': 203000,\n",
      " 'memory_efficient_fp16': False,\n",
      " 'min_loss_scale': 0.0001,\n",
      " 'min_lr': 1e-09,\n",
      " 'momentum': 0.99,\n",
      " 'no_epoch_checkpoints': False,\n",
      " 'no_progress_bar': True,\n",
      " 'no_save': False,\n",
      " 'no_token_positional_embeddings': False,\n",
      " 'num_workers': 0,\n",
      " 'optimizer': 'adam',\n",
      " 'optimizer_overrides': '{}',\n",
      " 'raw_text': False,\n",
      " 'relu_dropout': 0.0,\n",
      " 'reset_lr_scheduler': False,\n",
      " 'reset_optimizer': False,\n",
      " 'restore_file': 'checkpoint_last.pt',\n",
      " 'save_dir': '/checkpoint/edunov/20190403/wmt19ru2en.btsample5.ffn8192.transformer_wmt_en_de_big_bsz3584_lr0.0007_dr0.2_size_updates200000_seed2_lbsm0.1_size_sa0_upsample4/finetune/',\n",
      " 'save_interval': 1,\n",
      " 'save_interval_updates': 200,\n",
      " 'seed': 2,\n",
      " 'sentence_avg': False,\n",
      " 'share_all_embeddings': False,\n",
      " 'share_decoder_input_output_embed': True,\n",
      " 'skip_invalid_size_inputs_valid_test': False,\n",
      " 'source_lang': 'ru',\n",
      " 'target_lang': 'en',\n",
      " 'task': 'translation',\n",
      " 'tensorboard_logdir': '',\n",
      " 'threshold_loss_scale': None,\n",
      " 'train_subset': 'train',\n",
      " 'update_freq': [1],\n",
      " 'upsample_primary': 4,\n",
      " 'user_dir': None,\n",
      " 'valid_subset': 'valid',\n",
      " 'validate_interval': 1,\n",
      " 'warmup_init_lr': 1e-07,\n",
      " 'warmup_updates': 4000,\n",
      " 'weight_decay': 0.0}\n"
     ]
    }
   ],
   "source": [
    "pprint(vars(chkpt[\"args\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['no_progress_bar', 'log_interval', 'log_format', 'tensorboard_logdir', 'seed', 'cpu', 'fp16', 'memory_efficient_fp16', 'fp16_init_scale', 'fp16_scale_window', 'fp16_scale_tolerance', 'min_loss_scale', 'threshold_loss_scale', 'user_dir', 'task', 'num_workers', 'skip_invalid_size_inputs_valid_test', 'max_tokens', 'max_sentences', 'train_subset', 'valid_subset', 'max_sentences_valid', 'distributed_world_size', 'distributed_rank', 'distributed_backend', 'distributed_init_method', 'distributed_port', 'device_id', 'ddp_backend', 'bucket_cap_mb', 'fix_batches_to_gpus', 'arch', 'criterion', 'max_epoch', 'max_update', 'clip_norm', 'sentence_avg', 'update_freq', 'optimizer', 'lr', 'momentum', 'weight_decay', 'lr_scheduler', 'lr_shrink', 'min_lr', 'save_dir', 'restore_file', 'reset_optimizer', 'reset_lr_scheduler', 'optimizer_overrides', 'save_interval', 'save_interval_updates', 'keep_interval_updates', 'keep_last_epochs', 'no_save', 'no_epoch_checkpoints', 'validate_interval', 'no_token_positional_embeddings', 'label_smoothing', 'adam_betas', 'adam_eps', 'warmup_updates', 'warmup_init_lr', 'data', 'source_lang', 'target_lang', 'lazy_load', 'raw_text', 'left_pad_source', 'left_pad_target', 'max_source_positions', 'max_target_positions', 'upsample_primary', 'share_decoder_input_output_embed', 'dropout', 'encoder_ffn_embed_dim', 'attention_dropout', 'encoder_embed_dim', 'encoder_attention_heads', 'encoder_normalize_before', 'decoder_embed_dim', 'decoder_ffn_embed_dim', 'decoder_attention_heads', 'encoder_embed_path', 'encoder_layers', 'encoder_learned_pos', 'decoder_embed_path', 'decoder_layers', 'decoder_normalize_before', 'decoder_learned_pos', 'relu_dropout', 'adaptive_softmax_cutoff', 'adaptive_softmax_dropout', 'share_all_embeddings', 'adaptive_input', 'decoder_output_dim', 'decoder_input_dim'])\n"
     ]
    }
   ],
   "source": [
    "pprint(vars(chkpt[\"args\"]).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_fsmt import FSMTForConditionalGeneration\n",
    "from transformers.configuration_fsmt import FSMTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "hf_checkpoint_name = \"/code/huggingface/transformers-fair-wmt/data/wmt19-ru-en/config.json\"\n",
    "config = FSMTConfig.from_pretrained(hf_checkpoint_name)\n",
    "state_dict = chkpt[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chkpt[\"model\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renames/removal\n",
    "from collections import OrderedDict\n",
    "\n",
    "rename_keys = [\n",
    "#    (\"model.encoder.embed_positions._float_tensor\", \"model.encoder.embed_positions.weight\"),\n",
    "#    (\"model.decoder.embed_positions._float_tensor\", \"model.decoder.embed_positions.weight\"),\n",
    "#    (\"\", \"\"),\n",
    "#    (\"\", \"\"),\n",
    "#    (\"\", \"\"),\n",
    "#    (\"\", \"\"),    \n",
    "]\n",
    "\n",
    "def remove_ignore_keys_(state_dict):\n",
    "    ignore_keys = [\n",
    "        \"model.model\",\n",
    "        \"model.encoder.version\",\n",
    "        \"model.decoder.version\",\n",
    "        \"model.encoder.embed_positions._float_tensor\", # not storing model.encoder.embed_positions.weight\n",
    "        \"model.decoder.embed_positions._float_tensor\", # not storing model.decoder.embed_positions.weight\n",
    "    ]\n",
    "    for k in ignore_keys:\n",
    "        state_dict.pop(k, None)\n",
    "\n",
    "def rename_key(dct, old, new):\n",
    "    val = dct.pop(old)\n",
    "    dct[new] = val\n",
    "\n",
    "#state_dict = chkpt[\"model\"].copy()\n",
    "\n",
    "# rename keys to start with model.\n",
    "state_dict = OrderedDict((\"model.\"+k, v) for k, v in chkpt[\"model\"].items())\n",
    "# check:\n",
    "#state_dict[\"model.encoder.layers.0.fc1.bias\"]\n",
    "#chkpt[\"model\"][\"encoder.layers.0.fc1.bias\"]\n",
    "    \n",
    "remove_ignore_keys_(state_dict)\n",
    "for src, dest in rename_keys:\n",
    "    rename_key(state_dict, src, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pprint(state_dict.keys())\n",
    "#state_dict[\"model.encoder.layers.0.fc1.bias\"]\n",
    "#state_dict[\"model.encoder.embed_positions.weight\"]\n",
    "#torch.FloatTensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024 1024 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for FSMTForConditionalGeneration:\n\tMissing key(s) in state_dict: \"final_logits_bias\", \"model.encoder_embed_tokens.weight\", \"model.decoder_embed_tokens.weight\", \"model.encoder.embed_positions.weight\", \"model.encoder.layers.0.self_attn.k_proj.weight\", \"model.encoder.layers.0.self_attn.k_proj.bias\", \"model.encoder.layers.0.self_attn.v_proj.weight\", \"model.encoder.layers.0.self_attn.v_proj.bias\", \"model.encoder.layers.0.self_attn.q_proj.weight\", \"model.encoder.layers.0.self_attn.q_proj.bias\", \"model.encoder.layers.0.self_attn_layer_norm.weight\", \"model.encoder.layers.0.self_attn_layer_norm.bias\", \"model.encoder.layers.0.final_layer_norm.weight\", \"model.encoder.layers.0.final_layer_norm.bias\", \"model.encoder.layers.1.self_attn.k_proj.weight\", \"model.encoder.layers.1.self_attn.k_proj.bias\", \"model.encoder.layers.1.self_attn.v_proj.weight\", \"model.encoder.layers.1.self_attn.v_proj.bias\", \"model.encoder.layers.1.self_attn.q_proj.weight\", \"model.encoder.layers.1.self_attn.q_proj.bias\", \"model.encoder.layers.1.self_attn_layer_norm.weight\", \"model.encoder.layers.1.self_attn_layer_norm.bias\", \"model.encoder.layers.1.final_layer_norm.weight\", \"model.encoder.layers.1.final_layer_norm.bias\", \"model.encoder.layers.2.self_attn.k_proj.weight\", \"model.encoder.layers.2.self_attn.k_proj.bias\", \"model.encoder.layers.2.self_attn.v_proj.weight\", \"model.encoder.layers.2.self_attn.v_proj.bias\", \"model.encoder.layers.2.self_attn.q_proj.weight\", \"model.encoder.layers.2.self_attn.q_proj.bias\", \"model.encoder.layers.2.self_attn_layer_norm.weight\", \"model.encoder.layers.2.self_attn_layer_norm.bias\", \"model.encoder.layers.2.final_layer_norm.weight\", \"model.encoder.layers.2.final_layer_norm.bias\", \"model.encoder.layers.3.self_attn.k_proj.weight\", \"model.encoder.layers.3.self_attn.k_proj.bias\", \"model.encoder.layers.3.self_attn.v_proj.weight\", \"model.encoder.layers.3.self_attn.v_proj.bias\", \"model.encoder.layers.3.self_attn.q_proj.weight\", \"model.encoder.layers.3.self_attn.q_proj.bias\", \"model.encoder.layers.3.self_attn_layer_norm.weight\", \"model.encoder.layers.3.self_attn_layer_norm.bias\", \"model.encoder.layers.3.final_layer_norm.weight\", \"model.encoder.layers.3.final_layer_norm.bias\", \"model.encoder.layers.4.self_attn.k_proj.weight\", \"model.encoder.layers.4.self_attn.k_proj.bias\", \"model.encoder.layers.4.self_attn.v_proj.weight\", \"model.encoder.layers.4.self_attn.v_proj.bias\", \"model.encoder.layers.4.self_attn.q_proj.weight\", \"model.encoder.layers.4.self_attn.q_proj.bias\", \"model.encoder.layers.4.self_attn_layer_norm.weight\", \"model.encoder.layers.4.self_attn_layer_norm.bias\", \"model.encoder.layers.4.final_layer_norm.weight\", \"model.encoder.layers.4.final_layer_norm.bias\", \"model.encoder.layers.5.self_attn.k_proj.weight\", \"model.encoder.layers.5.self_attn.k_proj.bias\", \"model.encoder.layers.5.self_attn.v_proj.weight\", \"model.encoder.layers.5.self_attn.v_proj.bias\", \"model.encoder.layers.5.self_attn.q_proj.weight\", \"model.encoder.layers.5.self_attn.q_proj.bias\", \"model.encoder.layers.5.self_attn_layer_norm.weight\", \"model.encoder.layers.5.self_attn_layer_norm.bias\", \"model.encoder.layers.5.final_layer_norm.weight\", \"model.encoder.layers.5.final_layer_norm.bias\", \"model.encoder.layernorm_embedding.weight\", \"model.encoder.layernorm_embedding.bias\", \"model.decoder.embed_positions.weight\", \"model.decoder.layers.0.self_attn.k_proj.weight\", \"model.decoder.layers.0.self_attn.k_proj.bias\", \"model.decoder.layers.0.self_attn.v_proj.weight\", \"model.decoder.layers.0.self_attn.v_proj.bias\", \"model.decoder.layers.0.self_attn.q_proj.weight\", \"model.decoder.layers.0.self_attn.q_proj.bias\", \"model.decoder.layers.0.encoder_attn.k_proj.weight\", \"model.decoder.layers.0.encoder_attn.k_proj.bias\", \"model.decoder.layers.0.encoder_attn.v_proj.weight\", \"model.decoder.layers.0.encoder_attn.v_proj.bias\", \"model.decoder.layers.0.encoder_attn.q_proj.weight\", \"model.decoder.layers.0.encoder_attn.q_proj.bias\", \"model.decoder.layers.1.self_attn.k_proj.weight\", \"model.decoder.layers.1.self_attn.k_proj.bias\", \"model.decoder.layers.1.self_attn.v_proj.weight\", \"model.decoder.layers.1.self_attn.v_proj.bias\", \"model.decoder.layers.1.self_attn.q_proj.weight\", \"model.decoder.layers.1.self_attn.q_proj.bias\", \"model.decoder.layers.1.encoder_attn.k_proj.weight\", \"model.decoder.layers.1.encoder_attn.k_proj.bias\", \"model.decoder.layers.1.encoder_attn.v_proj.weight\", \"model.decoder.layers.1.encoder_attn.v_proj.bias\", \"model.decoder.layers.1.encoder_attn.q_proj.weight\", \"model.decoder.layers.1.encoder_attn.q_proj.bias\", \"model.decoder.layers.2.self_attn.k_proj.weight\", \"model.decoder.layers.2.self_attn.k_proj.bias\", \"model.decoder.layers.2.self_attn.v_proj.weight\", \"model.decoder.layers.2.self_attn.v_proj.bias\", \"model.decoder.layers.2.self_attn.q_proj.weight\", \"model.decoder.layers.2.self_attn.q_proj.bias\", \"model.decoder.layers.2.encoder_attn.k_proj.weight\", \"model.decoder.layers.2.encoder_attn.k_proj.bias\", \"model.decoder.layers.2.encoder_attn.v_proj.weight\", \"model.decoder.layers.2.encoder_attn.v_proj.bias\", \"model.decoder.layers.2.encoder_attn.q_proj.weight\", \"model.decoder.layers.2.encoder_attn.q_proj.bias\", \"model.decoder.layers.3.self_attn.k_proj.weight\", \"model.decoder.layers.3.self_attn.k_proj.bias\", \"model.decoder.layers.3.self_attn.v_proj.weight\", \"model.decoder.layers.3.self_attn.v_proj.bias\", \"model.decoder.layers.3.self_attn.q_proj.weight\", \"model.decoder.layers.3.self_attn.q_proj.bias\", \"model.decoder.layers.3.encoder_attn.k_proj.weight\", \"model.decoder.layers.3.encoder_attn.k_proj.bias\", \"model.decoder.layers.3.encoder_attn.v_proj.weight\", \"model.decoder.layers.3.encoder_attn.v_proj.bias\", \"model.decoder.layers.3.encoder_attn.q_proj.weight\", \"model.decoder.layers.3.encoder_attn.q_proj.bias\", \"model.decoder.layers.4.self_attn.k_proj.weight\", \"model.decoder.layers.4.self_attn.k_proj.bias\", \"model.decoder.layers.4.self_attn.v_proj.weight\", \"model.decoder.layers.4.self_attn.v_proj.bias\", \"model.decoder.layers.4.self_attn.q_proj.weight\", \"model.decoder.layers.4.self_attn.q_proj.bias\", \"model.decoder.layers.4.encoder_attn.k_proj.weight\", \"model.decoder.layers.4.encoder_attn.k_proj.bias\", \"model.decoder.layers.4.encoder_attn.v_proj.weight\", \"model.decoder.layers.4.encoder_attn.v_proj.bias\", \"model.decoder.layers.4.encoder_attn.q_proj.weight\", \"model.decoder.layers.4.encoder_attn.q_proj.bias\", \"model.decoder.layers.5.self_attn.k_proj.weight\", \"model.decoder.layers.5.self_attn.k_proj.bias\", \"model.decoder.layers.5.self_attn.v_proj.weight\", \"model.decoder.layers.5.self_attn.v_proj.bias\", \"model.decoder.layers.5.self_attn.q_proj.weight\", \"model.decoder.layers.5.self_attn.q_proj.bias\", \"model.decoder.layers.5.encoder_attn.k_proj.weight\", \"model.decoder.layers.5.encoder_attn.k_proj.bias\", \"model.decoder.layers.5.encoder_attn.v_proj.weight\", \"model.decoder.layers.5.encoder_attn.v_proj.bias\", \"model.decoder.layers.5.encoder_attn.q_proj.weight\", \"model.decoder.layers.5.encoder_attn.q_proj.bias\", \"model.decoder.layernorm_embedding.weight\", \"model.decoder.layernorm_embedding.bias\". \n\tUnexpected key(s) in state_dict: \"model.encoder.layers.0.layer_norms.0.weight\", \"model.encoder.layers.0.layer_norms.0.bias\", \"model.encoder.layers.0.layer_norms.1.weight\", \"model.encoder.layers.0.layer_norms.1.bias\", \"model.encoder.layers.0.self_attn.in_proj_weight\", \"model.encoder.layers.0.self_attn.in_proj_bias\", \"model.encoder.layers.1.layer_norms.0.weight\", \"model.encoder.layers.1.layer_norms.0.bias\", \"model.encoder.layers.1.layer_norms.1.weight\", \"model.encoder.layers.1.layer_norms.1.bias\", \"model.encoder.layers.1.self_attn.in_proj_weight\", \"model.encoder.layers.1.self_attn.in_proj_bias\", \"model.encoder.layers.2.layer_norms.0.weight\", \"model.encoder.layers.2.layer_norms.0.bias\", \"model.encoder.layers.2.layer_norms.1.weight\", \"model.encoder.layers.2.layer_norms.1.bias\", \"model.encoder.layers.2.self_attn.in_proj_weight\", \"model.encoder.layers.2.self_attn.in_proj_bias\", \"model.encoder.layers.3.layer_norms.0.weight\", \"model.encoder.layers.3.layer_norms.0.bias\", \"model.encoder.layers.3.layer_norms.1.weight\", \"model.encoder.layers.3.layer_norms.1.bias\", \"model.encoder.layers.3.self_attn.in_proj_weight\", \"model.encoder.layers.3.self_attn.in_proj_bias\", \"model.encoder.layers.4.layer_norms.0.weight\", \"model.encoder.layers.4.layer_norms.0.bias\", \"model.encoder.layers.4.layer_norms.1.weight\", \"model.encoder.layers.4.layer_norms.1.bias\", \"model.encoder.layers.4.self_attn.in_proj_weight\", \"model.encoder.layers.4.self_attn.in_proj_bias\", \"model.encoder.layers.5.layer_norms.0.weight\", \"model.encoder.layers.5.layer_norms.0.bias\", \"model.encoder.layers.5.layer_norms.1.weight\", \"model.encoder.layers.5.layer_norms.1.bias\", \"model.encoder.layers.5.self_attn.in_proj_weight\", \"model.encoder.layers.5.self_attn.in_proj_bias\", \"model.decoder.layers.0.self_attn.in_proj_weight\", \"model.decoder.layers.0.self_attn.in_proj_bias\", \"model.decoder.layers.0.encoder_attn.in_proj_weight\", \"model.decoder.layers.0.encoder_attn.in_proj_bias\", \"model.decoder.layers.1.self_attn.in_proj_weight\", \"model.decoder.layers.1.self_attn.in_proj_bias\", \"model.decoder.layers.1.encoder_attn.in_proj_weight\", \"model.decoder.layers.1.encoder_attn.in_proj_bias\", \"model.decoder.layers.2.self_attn.in_proj_weight\", \"model.decoder.layers.2.self_attn.in_proj_bias\", \"model.decoder.layers.2.encoder_attn.in_proj_weight\", \"model.decoder.layers.2.encoder_attn.in_proj_bias\", \"model.decoder.layers.3.self_attn.in_proj_weight\", \"model.decoder.layers.3.self_attn.in_proj_bias\", \"model.decoder.layers.3.encoder_attn.in_proj_weight\", \"model.decoder.layers.3.encoder_attn.in_proj_bias\", \"model.decoder.layers.4.self_attn.in_proj_weight\", \"model.decoder.layers.4.self_attn.in_proj_bias\", \"model.decoder.layers.4.encoder_attn.in_proj_weight\", \"model.decoder.layers.4.encoder_attn.in_proj_bias\", \"model.decoder.layers.5.self_attn.in_proj_weight\", \"model.decoder.layers.5.self_attn.in_proj_bias\", \"model.decoder.layers.5.encoder_attn.in_proj_weight\", \"model.decoder.layers.5.encoder_attn.in_proj_bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-27940832f503>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFSMTForConditionalGeneration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# show missing or extraneous/mismatching keys (need to remap/change model to match)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/main/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1052\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1053\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for FSMTForConditionalGeneration:\n\tMissing key(s) in state_dict: \"final_logits_bias\", \"model.encoder_embed_tokens.weight\", \"model.decoder_embed_tokens.weight\", \"model.encoder.embed_positions.weight\", \"model.encoder.layers.0.self_attn.k_proj.weight\", \"model.encoder.layers.0.self_attn.k_proj.bias\", \"model.encoder.layers.0.self_attn.v_proj.weight\", \"model.encoder.layers.0.self_attn.v_proj.bias\", \"model.encoder.layers.0.self_attn.q_proj.weight\", \"model.encoder.layers.0.self_attn.q_proj.bias\", \"model.encoder.layers.0.self_attn_layer_norm.weight\", \"model.encoder.layers.0.self_attn_layer_norm.bias\", \"model.encoder.layers.0.final_layer_norm.weight\", \"model.encoder.layers.0.final_layer_norm.bias\", \"model.encoder.layers.1.self_attn.k_proj.weight\", \"model.encoder.layers.1.self_attn.k_proj.bias\", \"model.encoder.layers.1.self_attn.v_proj.weight\", \"model.encoder.layers.1.self_attn.v_proj.bias\", \"model.encoder.layers.1.self_attn.q_proj.weight\", \"model.encoder.layers.1.self_attn.q_proj.bias\", \"model.encoder.layers.1.self_attn_layer_norm.weight\", \"model.encoder.layers.1.self_attn_layer_norm.bias\", \"model.encoder.layers.1.final_layer_norm.weight\", \"model.encoder.layers.1.final_layer_norm.bias\", \"model.encoder.layers.2.self_attn.k_proj.weight\", \"model.encoder.layers.2.self_attn.k_proj.bias\", \"model.encoder.layers.2.self_attn.v_proj.weight\", \"model.encoder.layers.2.self_attn.v_proj.bias\", \"model.encoder.layers.2.self_attn.q_proj.weight\", \"model.encoder.layers.2.self_attn.q_proj.bias\", \"model.encoder.layers.2.self_attn_layer_norm.weight\", \"model.encoder.layers.2.self_attn_layer_norm.bias\", \"model.encoder.layers.2.final_layer_norm.weight\", \"model.encoder.layers.2.final_layer_norm.bias\", \"model.encoder.layers.3.self_attn.k_proj.weight\", \"model.encoder.layers.3.self_attn.k_proj.bias\", \"model.encoder.layers.3.self_attn.v_proj.weight\", \"model.encoder.layers.3.self_attn.v_proj.bias\", \"model.encoder.layers.3.self_attn.q_proj.weight\", \"model.encoder.layers.3.self_attn.q_proj.bias\", \"model.encoder.layers.3.self_attn_layer_norm.weight\", \"model.encoder.layers.3.self_attn_layer_norm.bias\", \"model.encoder.layers.3.final_layer_norm.weight\", \"model.encoder.layers.3.final_layer_norm.bias\", \"model.encoder.layers.4.self_attn.k_proj.weight\", \"model.encoder.layers.4.self_attn.k_proj.bias\", \"model.encoder.layers.4.self_attn.v_proj.weight\", \"model.encoder.layers.4.self_attn.v_proj.bias\", \"model.encoder.layers.4.self_attn.q_proj.weight\", \"model.encoder.layers.4.self_attn.q_proj.bias\", \"model.encoder.layers.4.self_attn_layer_norm.weight\", \"model.encoder.layers.4.self_attn_layer_norm.bias\", \"model.encoder.layers.4.final_layer_norm.weight\", \"model.encoder.layers.4.final_layer_norm.bias\", \"model.encoder.layers.5.self_attn.k_proj.weight\", \"model.encoder.layers.5.self_attn.k_proj.bias\", \"model.encoder.layers.5.self_attn.v_proj.weight\", \"model.encoder.layers.5.self_attn.v_proj.bias\", \"model.encoder.layers.5.self_attn.q_proj.weight\", \"model.encoder.layers.5.self_attn.q_proj.bias\", \"model.encoder.layers.5.self_attn_layer_norm.weight\", \"model.encoder.layers.5.self_attn_layer_norm.bias\", \"model.encoder.layers.5.final_layer_norm.weight\", \"model.encoder.layers.5.final_layer_norm.bias\", \"model.encoder.layernorm_embedding.weight\", \"model.encoder.layernorm_embedding.bias\", \"model.decoder.embed_positions.weight\", \"model.decoder.layers.0.self_attn.k_proj.weight\", \"model.decoder.layers.0.self_attn.k_proj.bias\", \"model.decoder.layers.0.self_attn.v_proj.weight\", \"model.decoder.layers.0.self_attn.v_proj.bias\", \"model.decoder.layers.0.self_attn.q_proj.weight\", \"model.decoder.layers.0.self_attn.q_proj.bias\", \"model.decoder.layers.0.encoder_attn.k_proj.weight\", \"model.decoder.layers.0.encoder_attn.k_proj.bias\", \"model.decoder.layers.0.encoder_attn.v_proj.weight\", \"model.decoder.layers.0.encoder_attn.v_proj.bias\", \"model.decoder.layers.0.encoder_attn.q_proj.weight\", \"model.decoder.layers.0.encoder_attn.q_proj.bias\", \"model.decoder.layers.1.self_attn.k_proj.weight\", \"model.decoder.layers.1.self_attn.k_proj.bias\", \"model.decoder.layers.1.self_attn.v_proj.weight\", \"model.decoder.layers.1.self_attn.v_proj.bias\", \"model.decoder.layers.1.self_attn.q_proj.weight\", \"model.decoder.layers.1.self_attn.q_proj.bias\", \"model.decoder.layers.1.encoder_attn.k_proj.weight\", \"model.decoder.layers.1.encoder_attn.k_proj.bias\", \"model.decoder.layers.1.encoder_attn.v_proj.weight\", \"model.decoder.layers.1.encoder_attn.v_proj.bias\", \"model.decoder.layers.1.encoder_attn.q_proj.weight\", \"model.decoder.layers.1.encoder_attn.q_proj.bias\", \"model.decoder.layers.2.self_attn.k_proj.weight\", \"model.decoder.layers.2.self_attn.k_proj.bias\", \"model.decoder.layers.2.self_attn.v_proj.weight\", \"model.decoder.layers.2.self_attn.v_proj.bias\", \"model.decoder.layers.2.self_attn.q_proj.weight\", \"model.decoder.layers.2.self_attn.q_proj.bias\", \"model.decoder.layers.2.encoder_attn.k_proj.weight\", \"model.decoder.layers.2.encoder_attn.k_proj.bias\", \"model.decoder.layers.2.encoder_attn.v_proj.weight\", \"model.decoder.layers.2.encoder_attn.v_proj.bias\", \"model.decoder.layers.2.encoder_attn.q_proj.weight\", \"model.decoder.layers.2.encoder_attn.q_proj.bias\", \"model.decoder.layers.3.self_attn.k_proj.weight\", \"model.decoder.layers.3.self_attn.k_proj.bias\", \"model.decoder.layers.3.self_attn.v_proj.weight\", \"model.decoder.layers.3.self_attn.v_proj.bias\", \"model.decoder.layers.3.self_attn.q_proj.weight\", \"model.decoder.layers.3.self_attn.q_proj.bias\", \"model.decoder.layers.3.encoder_attn.k_proj.weight\", \"model.decoder.layers.3.encoder_attn.k_proj.bias\", \"model.decoder.layers.3.encoder_attn.v_proj.weight\", \"model.decoder.layers.3.encoder_attn.v_proj.bias\", \"model.decoder.layers.3.encoder_attn.q_proj.weight\", \"model.decoder.layers.3.encoder_attn.q_proj.bias\", \"model.decoder.layers.4.self_attn.k_proj.weight\", \"model.decoder.layers.4.self_attn.k_proj.bias\", \"model.decoder.layers.4.self_attn.v_proj.weight\", \"model.decoder.layers.4.self_attn.v_proj.bias\", \"model.decoder.layers.4.self_attn.q_proj.weight\", \"model.decoder.layers.4.self_attn.q_proj.bias\", \"model.decoder.layers.4.encoder_attn.k_proj.weight\", \"model.decoder.layers.4.encoder_attn.k_proj.bias\", \"model.decoder.layers.4.encoder_attn.v_proj.weight\", \"model.decoder.layers.4.encoder_attn.v_proj.bias\", \"model.decoder.layers.4.encoder_attn.q_proj.weight\", \"model.decoder.layers.4.encoder_attn.q_proj.bias\", \"model.decoder.layers.5.self_attn.k_proj.weight\", \"model.decoder.layers.5.self_attn.k_proj.bias\", \"model.decoder.layers.5.self_attn.v_proj.weight\", \"model.decoder.layers.5.self_attn.v_proj.bias\", \"model.decoder.layers.5.self_attn.q_proj.weight\", \"model.decoder.layers.5.self_attn.q_proj.bias\", \"model.decoder.layers.5.encoder_attn.k_proj.weight\", \"model.decoder.layers.5.encoder_attn.k_proj.bias\", \"model.decoder.layers.5.encoder_attn.v_proj.weight\", \"model.decoder.layers.5.encoder_attn.v_proj.bias\", \"model.decoder.layers.5.encoder_attn.q_proj.weight\", \"model.decoder.layers.5.encoder_attn.q_proj.bias\", \"model.decoder.layernorm_embedding.weight\", \"model.decoder.layernorm_embedding.bias\". \n\tUnexpected key(s) in state_dict: \"model.encoder.layers.0.layer_norms.0.weight\", \"model.encoder.layers.0.layer_norms.0.bias\", \"model.encoder.layers.0.layer_norms.1.weight\", \"model.encoder.layers.0.layer_norms.1.bias\", \"model.encoder.layers.0.self_attn.in_proj_weight\", \"model.encoder.layers.0.self_attn.in_proj_bias\", \"model.encoder.layers.1.layer_norms.0.weight\", \"model.encoder.layers.1.layer_norms.0.bias\", \"model.encoder.layers.1.layer_norms.1.weight\", \"model.encoder.layers.1.layer_norms.1.bias\", \"model.encoder.layers.1.self_attn.in_proj_weight\", \"model.encoder.layers.1.self_attn.in_proj_bias\", \"model.encoder.layers.2.layer_norms.0.weight\", \"model.encoder.layers.2.layer_norms.0.bias\", \"model.encoder.layers.2.layer_norms.1.weight\", \"model.encoder.layers.2.layer_norms.1.bias\", \"model.encoder.layers.2.self_attn.in_proj_weight\", \"model.encoder.layers.2.self_attn.in_proj_bias\", \"model.encoder.layers.3.layer_norms.0.weight\", \"model.encoder.layers.3.layer_norms.0.bias\", \"model.encoder.layers.3.layer_norms.1.weight\", \"model.encoder.layers.3.layer_norms.1.bias\", \"model.encoder.layers.3.self_attn.in_proj_weight\", \"model.encoder.layers.3.self_attn.in_proj_bias\", \"model.encoder.layers.4.layer_norms.0.weight\", \"model.encoder.layers.4.layer_norms.0.bias\", \"model.encoder.layers.4.layer_norms.1.weight\", \"model.encoder.layers.4.layer_norms.1.bias\", \"model.encoder.layers.4.self_attn.in_proj_weight\", \"model.encoder.layers.4.self_attn.in_proj_bias\", \"model.encoder.layers.5.layer_norms.0.weight\", \"model.encoder.layers.5.layer_norms.0.bias\", \"model.encoder.layers.5.layer_norms.1.weight\", \"model.encoder.layers.5.layer_norms.1.bias\", \"model.encoder.layers.5.self_attn.in_proj_weight\", \"model.encoder.layers.5.self_attn.in_proj_bias\", \"model.decoder.layers.0.self_attn.in_proj_weight\", \"model.decoder.layers.0.self_attn.in_proj_bias\", \"model.decoder.layers.0.encoder_attn.in_proj_weight\", \"model.decoder.layers.0.encoder_attn.in_proj_bias\", \"model.decoder.layers.1.self_attn.in_proj_weight\", \"model.decoder.layers.1.self_attn.in_proj_bias\", \"model.decoder.layers.1.encoder_attn.in_proj_weight\", \"model.decoder.layers.1.encoder_attn.in_proj_bias\", \"model.decoder.layers.2.self_attn.in_proj_weight\", \"model.decoder.layers.2.self_attn.in_proj_bias\", \"model.decoder.layers.2.encoder_attn.in_proj_weight\", \"model.decoder.layers.2.encoder_attn.in_proj_bias\", \"model.decoder.layers.3.self_attn.in_proj_weight\", \"model.decoder.layers.3.self_attn.in_proj_bias\", \"model.decoder.layers.3.encoder_attn.in_proj_weight\", \"model.decoder.layers.3.encoder_attn.in_proj_bias\", \"model.decoder.layers.4.self_attn.in_proj_weight\", \"model.decoder.layers.4.self_attn.in_proj_bias\", \"model.decoder.layers.4.encoder_attn.in_proj_weight\", \"model.decoder.layers.4.encoder_attn.in_proj_bias\", \"model.decoder.layers.5.self_attn.in_proj_weight\", \"model.decoder.layers.5.self_attn.in_proj_bias\", \"model.decoder.layers.5.encoder_attn.in_proj_weight\", \"model.decoder.layers.5.encoder_attn.in_proj_bias\". "
     ]
    }
   ],
   "source": [
    "#%%pixie_debugger -b /code/huggingface/transformers-fair-wmt/src/transformers/modeling_fsmt.py:913\n",
    "\n",
    "model = FSMTForConditionalGeneration(config).eval()\n",
    "# show missing or extraneous/mismatching keys (need to remap/change model to match)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "emb = nn.Embedding(2, 1, 0)\n",
    "#t1 = emb(torch.LongTensor([0]))\n",
    "#t1.shape\n",
    "emb.num_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.tokenization_fsmt import FSMTTokenizer\n",
    "from transformers.modeling_fsmt import FSMTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Success!!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "authorized_missing_keys = [r\"final_logits_bias\", r\"encoder\\.version\", r\"decoder\\.version\"]\n",
    "missing_keys = [\"final_logits_bias\"]\n",
    "for pat in authorized_missing_keys:\n",
    "    missing_keys = [k for k in missing_keys if re.search(pat, k) is None]\n",
    "missing_keys"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "240.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
